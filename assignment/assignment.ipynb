{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting and dividing the data\n",
    "\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import os\n",
    "dirname = \"Warwick QU Dataset (Released 2016_07_08)/\"\n",
    "reshapedirname = \"reshaped_warwick/\"\n",
    "final_height = 400\n",
    "final_width = 600\n",
    "try:\n",
    "\tos.stat(reshapedirname)\n",
    "except:\n",
    "\tos.mkdir(reshapedirname)\n",
    "for subdirname in [\"segments_train\",\"segments_test\",\"segments_train_eval\",\"images_train\",\"images_test\",\"images_train_eval\"]:\n",
    "\ttry:\n",
    "\t\tos.stat(reshapedirname+subdirname)\n",
    "\texcept:\n",
    "\t\tos.mkdir(reshapedirname+subdirname)\n",
    "prefix_name = [\"images\",\"segments\"]\n",
    "suffix_name = [\"train\",\"test\"]\n",
    "for file in os.listdir(dirname):\n",
    "\tif file.endswith(\".bmp\"):\n",
    "\t\timg = Image.open(dirname+file)\n",
    "\t\twidth, height = img.size\n",
    "\t\tistest,isanno = False,False\n",
    "\t\tif file[:4] == \"test\":\n",
    "\t\t\tistest = True\n",
    "\t\tif file.find(\"anno\") != -1:\n",
    "\t\t\tisanno = True\n",
    "\t\tif isanno:\n",
    "\t\t\t# change the image to saturated\n",
    "\t\t\timg = img.point(lambda i: i * 255)\n",
    "\t\treshaped_image = img = img.resize((final_width, final_height), PIL.Image.ANTIALIAS)\n",
    "\t\tnewfilename = reshapedirname+prefix_name[isanno]+\"_\"+suffix_name[istest]+\"/\"+file.split('.')[0]\n",
    "\t\tnewfilenamefull = newfilename+\".png\"\n",
    "\t\treshaped_image.save(newfilenamefull,\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for decoding the Dataset binary file format\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Process images of this size.\n",
    "IMAGE_WIDTH = 600\n",
    "IMAGE_HEIGHT = 400\n",
    "\n",
    "# Global constants describing the Dataset data set.\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 320\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 20\n",
    "\n",
    "\n",
    "def read_from_queue(path,num_channels):\n",
    "    return tf.image.decode_png(path, channels=0, dtype=tf.uint16)\n",
    "\n",
    "\n",
    "def read_dataset(all_files_queue):\n",
    "\n",
    "    class DatasetRecord(object):\n",
    "        pass\n",
    "\n",
    "    result = DatasetRecord()\n",
    "\n",
    "    # Read a record, getting filenames from the filename_queue.\n",
    "    text_reader = tf.TextLineReader()\n",
    "    _, csv_content = text_reader.read(all_files_queue)\n",
    "\n",
    "    i_path, s_path = tf.decode_csv(csv_content,\n",
    "                                           record_defaults=[[\"\"], [\"\"]])\n",
    "\n",
    "    result.uint8image = read_from_queue(tf.read_file(i_path),0)\n",
    "    segment = read_from_queue(tf.read_file(s_path),0)\n",
    "\n",
    "    result.label = segment\n",
    "    result.i_path = i_path\n",
    "    result.s_path = s_path\n",
    "    result.csv = csv_content\n",
    "    return result\n",
    "\n",
    "\n",
    "def _generate_image_and_label_batch(image, label, i_path, min_queue_examples,\n",
    "                                    batch_size, shuffle):\n",
    "    \n",
    "    # Create a queue that shuffles the examples, and then\n",
    "    # read 'batch_size' images + labels from the example queue.\n",
    "    num_preprocess_threads = 16\n",
    "    images, labels, i_paths = tf.train.batch(\n",
    "        [image, label, i_path],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples+batch_size)\n",
    "\n",
    "    # Display the training images in the visualizer.\n",
    "    tf.summary.image('images', images)\n",
    "\n",
    "    return images, labels, i_paths\n",
    "\n",
    "\n",
    "def gen_csv_paths(data_dir, pref, sessid = None):\n",
    "    \n",
    "    filenames = get_png_files(os.path.join(data_dir, 'images_' + pref))\n",
    "    # segments = get_png_files(os.path.join(data_dir, 'segments_'+ pref))\n",
    "    # Assuming that segments have just anno added to their names\n",
    "\n",
    "    filenames.sort()\n",
    "    segments = []\n",
    "    for filename in filenames:\n",
    "        name = filename\n",
    "        name = name.replace(\"/images_\",\"/segments_\")\n",
    "        segments.append(name[:-4]+\"_anno\"+name[-4:])\n",
    "    all_files = np.array([filenames, segments])\n",
    "\n",
    "    if pref == 'train':\n",
    "        indices = [random.randint(0,len(all_files[0])-1) for i in range(len(all_files[0]))]\n",
    "        all_files = all_files[:,indices]\n",
    "        pd_arr = pd.DataFrame(all_files).transpose()\n",
    "        pd_arr.to_csv(pref + str(sessid) + '.csv', index=False, header=False)\n",
    "    else:\n",
    "        pd_arr = pd.DataFrame(all_files).transpose()\n",
    "        pd_arr.to_csv(pref + '.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "def get_read_input(eval_data, sessid = None):\n",
    "    \n",
    "    # Create queues that produce the filenames and labels to read.\n",
    "    if eval_data == 'train':\n",
    "        all_files_queue = tf.train.string_input_producer([eval_data + str(sessid) + '.csv'])\n",
    "    else:\n",
    "        all_files_queue = tf.train.string_input_producer([eval_data + '.csv'])\n",
    "\n",
    "    # Read examples from files in the filename queue.\n",
    "    read_input = read_dataset(all_files_queue)\n",
    "    reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "    read_input.label = tf.cast(read_input.label, tf.int32)\n",
    "\n",
    "    return read_input, reshaped_image\n",
    "\n",
    "\n",
    "def get_png_files(dirname):\n",
    "    return [dirname + '/' + f for f in os.listdir(dirname) if f.endswith('.png')]\n",
    "\n",
    "\n",
    "def inputs(eval_data, batch_size, sessid):\n",
    "    \n",
    "    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "    read_input, reshaped_image = get_read_input(eval_data, sessid)\n",
    "\n",
    "    # Image processing for evaluation.\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(reshaped_image)\n",
    "\n",
    "    # Set the shapes of tensors.\n",
    "    float_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
    "    read_input.label.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, 1])\n",
    "\n",
    "    # Set max intensity to 1\n",
    "    read_input.label = tf.cast(tf.divide(read_input.label, 255), tf.int32)\n",
    "\n",
    "    # Ensure that the random shuffling has good mixing properties.\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    min_queue_examples = int(num_examples_per_epoch *\n",
    "                             min_fraction_of_examples_in_queue)\n",
    "\n",
    "    # Generate a batch of images and labels by building up a queue of examples.\n",
    "    shuffle = False\n",
    "    # shuffle = False if eval_data == 'test' else True\n",
    "    return _generate_image_and_label_batch(float_image, read_input.label, read_input.i_path,\n",
    "                                           min_queue_examples, batch_size,\n",
    "                                           shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the network\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.slim.python.slim.nets.resnet_v2 import bottleneck\n",
    "import numpy as np\n",
    "import math\n",
    "import data_input\n",
    "\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_utils\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Basic model parameters.\n",
    "tf.app.flags.DEFINE_integer('batch_size', 1)\n",
    "tf.app.flags.DEFINE_string('data_dir', 'data')\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False)\n",
    "tf.app.flags.DEFINE_integer('num_layers', 6)\n",
    "tf.app.flags.DEFINE_integer('num_classes', 2)\n",
    "tf.app.flags.DEFINE_integer('feat_root', 32)\n",
    "tf.app.flags.DEFINE_integer('deconv_root', 8)\n",
    "\n",
    "# Global constants describing the BBBC006 data set.\n",
    "IMAGE_WIDTH = data_input.IMAGE_WIDTH\n",
    "IMAGE_HEIGHT = data_input.IMAGE_HEIGHT\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = data_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = data_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9995  # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 100.0  # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.01  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.05  # Initial learning rate.\n",
    "DROPOUT_RATE = 0.5  # Probability for dropout layers.\n",
    "S_CLASS_PROP = .2249  # Segments proportion of pixels in class 1.\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "\n",
    "def _activation_summary(x):\n",
    "\t# Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "\t# session. This helps the clarity of presentation on tensorboard.\n",
    "\ttensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "\ttf.summary.histogram(tensor_name + '/activations', x)\n",
    "\ttf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "\tdtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "\tvar = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "\treturn var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "\tdtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "\tvar = _variable_on_cpu(\n",
    "\t\tname,\n",
    "\t\tshape,\n",
    "\t\ttf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "\tif wd is not None and not tf.get_variable_scope().reuse:\n",
    "\t\tweight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "\t\ttf.add_to_collection('losses', weight_decay)\n",
    "\treturn var\n",
    "\n",
    "\n",
    "def distorted_inputs():\n",
    "\tif not FLAGS.data_dir:\n",
    "\t\traise ValueError('Please supply a data_dir')\n",
    "\timages, labels = data_input.distorted_inputs(batch_size=FLAGS.batch_size)\n",
    "\tif FLAGS.use_fp16:\n",
    "\t\timages = tf.cast(images, tf.float16)\n",
    "\t\tlabels = tf.cast(labels, tf.float16)\n",
    "\treturn images, labels\n",
    "\n",
    "\n",
    "def inputs(eval_data, sessid):\n",
    "\tif not FLAGS.data_dir:\n",
    "\t\traise ValueError('Please supply a data_dir')\n",
    "\timages, labels, i_paths = data_input.inputs(eval_data=eval_data,\n",
    "\t\t\t\t\t\t\t\t\t\t  batch_size=FLAGS.batch_size, sessid = sessid)\n",
    "\tlabels = tf.cast(tf.divide(labels,255),tf.int32)\n",
    "\tif FLAGS.use_fp16:\n",
    "\t\timages = tf.cast(images, tf.float16)\n",
    "\t\tlabels = tf.cast(labels, tf.float16)\n",
    "\treturn images, labels, i_paths\n",
    "\n",
    "\n",
    "def get_deconv_filter(shape):\n",
    "\twidth = shape[0]\n",
    "\theight = shape[0]\n",
    "\tf = math.ceil(width / 2.0)\n",
    "\tc = (2.0 * f - 1 - f % 2) / (2.0 * f)\n",
    "\n",
    "\tbilinear = np.zeros([shape[0], shape[1]])\n",
    "\tfor x in range(width):\n",
    "\t\tfor y in range(height):\n",
    "\t\t\tbilinear[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n",
    "\n",
    "\tweights = np.zeros(shape)\n",
    "\tfor i in range(shape[2]):\n",
    "\t\tweights[:, :, i, i] = bilinear\n",
    "\n",
    "\tinit = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
    "\treturn tf.get_variable(name='up_filter', initializer=init, shape=weights.shape)\n",
    "\n",
    "\n",
    "def _deconv_layer(in_layer, w, b, dc, ds, scope):\n",
    "\tdeconv = tf.nn.conv2d_transpose(in_layer, w, ds, strides=[1, dc, dc, 1],\n",
    "\t\t\t\t\t\t\t\t\tpadding='SAME')\n",
    "\tdeconv = tf.nn.bias_add(deconv, bias=b, name=scope.name)\n",
    "\tdeconv = tf.nn.relu(deconv)\n",
    "\t_activation_summary(deconv)\n",
    "\treturn deconv\n",
    "\n",
    "\n",
    "def inference(images, train=True):\n",
    "\tfeat_out = FLAGS.feat_root\n",
    "\t# in_layer = tf.layers.batch_normalization(images)\n",
    "\tin_layer = images\n",
    "\n",
    "\t# Deconvolution constant: kernel size = 2 * dc, stride = dc\n",
    "\t# Deconvolution output shape\n",
    "\tdc = FLAGS.deconv_root\n",
    "\tds = [FLAGS.batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, FLAGS.num_classes]\n",
    "\n",
    "\t# Up-sampled layers 4-6 output maps for contours and segments, respectively\n",
    "\ts_outputs = []\n",
    "\n",
    "\tfor layer in range(FLAGS.num_layers):\n",
    "\t\t# CONVOLUTION\n",
    "\t\twith tf.variable_scope('conv{}'.format(layer + 1)) as scope:\n",
    "\t\t\t# Double the number of feat_out for all but convolution layer 4\n",
    "\t\t\tfeat_out *= 2 if layer != 4 else 1\n",
    "\t\t\tconv = tf.layers.conv2d(in_layer, feat_out, (3, 3), padding='same',\n",
    "\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu, name=scope.name)\n",
    "\n",
    "\t\t\tif train and layer > 3:  # During training, add dropout to layers 5 and 6\n",
    "\t\t\t\tconv = tf.nn.dropout(conv, keep_prob=DROPOUT_RATE)\n",
    "\n",
    "\t\t\t_activation_summary(conv)\n",
    "\n",
    "\t\t# POOLING\n",
    "\t\t# First and convolution layers has no pooling afterwards\n",
    "\t\tif 0 < layer:\n",
    "\t\t\tpool = tf.layers.max_pooling2d(conv, 2, 2, padding='same')\n",
    "\n",
    "\t\t\t_activation_summary(pool)\n",
    "\t\t\tin_layer = pool\n",
    "\t\telse:\n",
    "\t\t\tin_layer = conv\n",
    "\n",
    "\t\t# Transposed convolution and output mapping for segments and contours\n",
    "\t\tif layer > 2:  # Only applies to layers 3-5\n",
    "\t\t\t# TRANSPOSED CONVOLUTION\n",
    "\t\t\twith tf.variable_scope('deconv{0}'.format(layer + 1)) as scope:\n",
    "\t\t\t\tfeat_in = in_layer.get_shape().as_list()[-1]\n",
    "\t\t\t\tshape = [dc * 2, dc * 2, FLAGS.num_classes, feat_in]\n",
    "\t\t\t\tw = get_deconv_filter(shape)\n",
    "\t\t\t\tb = _variable_on_cpu('biases', [FLAGS.num_classes],\n",
    "\t\t\t\t\t\t\ttf.constant_initializer(0.1))\n",
    "\n",
    "\t\t\t\tdeconv = _deconv_layer(in_layer, w, b, dc, ds, scope)\n",
    "\n",
    "\t\t\twith tf.variable_scope('output{0}'.format(layer + 1)) as scope:\n",
    "\t\t\t\toutput = tf.layers.conv2d(deconv, FLAGS.num_classes, (1, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t  padding='same', activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t\t\t\t  name=scope.name)\n",
    "\t\t\t\ts_outputs.append(output)\n",
    "\t\t\tdc *= 2\n",
    "\ts_fuse = tf.add_n(s_outputs)\n",
    "\n",
    "\treturn s_fuse\n",
    "\n",
    "\"\"\"\n",
    "\tCopied from tf website, just tweaked a bit to include BN and ReLU\n",
    "\"\"\"\n",
    "def bottleneck(inputs,\n",
    "               depth,\n",
    "               depth_bottleneck,\n",
    "               stride,\n",
    "               rate=1,\n",
    "               outputs_collections=None,\n",
    "               scope=None):\n",
    "  \n",
    "  with variable_scope.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:\n",
    "    depth_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "    preact = layers.batch_norm(\n",
    "        inputs, activation_fn=nn_ops.relu, scope='preact')\n",
    "    if depth == depth_in:\n",
    "      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n",
    "    else:\n",
    "      shortcut = layers_lib.conv2d(\n",
    "          preact,\n",
    "          depth, [1, 1],\n",
    "          stride=stride,\n",
    "          normalizer_fn=None,\n",
    "          activation_fn=None,\n",
    "          scope='shortcut')\n",
    "\n",
    "    residual = preact\n",
    "    residual = tf.layers.batch_normalization(residual)\n",
    "    residual = tf.nn.relu(residual)\n",
    "    residual = layers_lib.conv2d(\n",
    "        residual, depth_bottleneck, [1, 1], stride=1, scope='conv1')\n",
    "    residual = tf.layers.batch_normalization(residual)\n",
    "    residual = tf.nn.relu(residual)\n",
    "    residual = resnet_utils.conv2d_same(\n",
    "        residual, depth_bottleneck, 3, stride, rate=rate, scope='conv2')\n",
    "    residual = tf.layers.batch_normalization(residual)\n",
    "    residual = tf.nn.relu(residual)\n",
    "    residual = layers_lib.conv2d(\n",
    "        residual,\n",
    "        depth, [1, 1],\n",
    "        stride=1,\n",
    "        normalizer_fn=None,\n",
    "        activation_fn=None,\n",
    "        scope='conv3')\n",
    "\n",
    "    output = shortcut + residual\n",
    "\n",
    "    return utils.collect_named_outputs(outputs_collections, sc.name, output)\n",
    "\n",
    "def inference_bottleneck(images, train=True):\n",
    "\tin_layer = images\n",
    "\tfeat_out = FLAGS.feat_root\n",
    "\ts_outputs = []\n",
    "\t\n",
    "\twith tf.variable_scope('bottleneck0-1') as scope:\n",
    "\t\tin_layer = tf.layers.max_pooling2d(in_layer, 2, 2, padding='same')\n",
    "\t\tin_layer = tf.layers.batch_normalization(in_layer)\n",
    "\t\tin_layer = tf.nn.relu(in_layer)\n",
    "\t\tin_layer = tf.layers.conv2d(in_layer, feat_out, (3, 3), padding='same', name=scope.name)\n",
    "\twith tf.variable_scope('bottleneck0-2') as scope:\n",
    "\t\tin_layer = tf.layers.batch_normalization(in_layer)\n",
    "\t\tin_layer = tf.nn.relu(in_layer)\n",
    "\t\tin_layer = tf.layers.conv2d(in_layer, feat_out, (3, 3), padding='same', name=scope.name)\n",
    "\t\ts_outputs.append(in_layer)\n",
    "\tfor layer in range(5):\n",
    "\t\twith tf.variable_scope('bottleneck{0}-{1}'.format(layer + 1,1)) as scope:\n",
    "\t\t\tin_layer = tf.layers.max_pooling2d(in_layer, 2, 2, padding='same')\n",
    "\t\t\tin_layer = bottleneck(in_layer,min(feat_out*2,256),feat_out,1)\n",
    "\t\twith tf.variable_scope('bottleneck{0}-{1}'.format(layer + 1,2)) as scope:\n",
    "\t\t\tfeat_out = min(feat_out*2,256)\n",
    "\t\t\tin_layer = bottleneck(in_layer,feat_out,feat_out,1)\n",
    "\t\t\ts_outputs.append(in_layer)\n",
    "\t# populate s_outputs\n",
    "\tencoding = in_layer\n",
    "\tdc = 2 \n",
    "\tds = [FLAGS.batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, FLAGS.num_classes]\n",
    "\tfor layer in range(len(s_outputs)):\n",
    "\t\twith tf.variable_scope('deconv{0}'.format(layer + 1)) as scope:\n",
    "\t\t\tin_layer = s_outputs[layer]\n",
    "\t\t\tfeat_in = in_layer.get_shape().as_list()[-1]\n",
    "\t\t\tshape = [dc * 2, dc * 2, FLAGS.num_classes, feat_in]\n",
    "\t\t\tw = get_deconv_filter(shape)\n",
    "\t\t\tb = _variable_on_cpu('biases', [FLAGS.num_classes],\n",
    "\t\t\t\t\t\t\t\t tf.constant_initializer(0.1))\n",
    "\t\t\tdeconv = _deconv_layer(in_layer, w, b, dc, ds, scope)\n",
    "\t\t\ts_outputs[layer] = deconv\n",
    "\t\tdc *= 2\n",
    "\n",
    "\ts_fuse = tf.concat(s_outputs,axis=-1)\n",
    "\twith tf.variable_scope('after_fusion-1') as scope:\n",
    "\t\ts_fuse = tf.layers.batch_normalization(s_fuse)\n",
    "\t\ts_fuse = tf.nn.relu(s_fuse)\n",
    "\t\ts_fuse = tf.layers.conv2d(s_fuse, 2, (3, 3), padding='same', name=scope.name)\n",
    "\twith tf.variable_scope('after_fusion-2') as scope:\n",
    "\t\ts_fuse = tf.layers.batch_normalization(s_fuse)\n",
    "\t\ts_fuse = tf.nn.relu(s_fuse)\n",
    "\t\ts_fuse = tf.layers.conv2d(s_fuse, 2, (1, 1), padding='same', name=scope.name)\n",
    "\treturn s_fuse, encoding\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "\t# Compute the moving average of all individual losses and the total loss.\n",
    "\tloss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "\tlosses = tf.get_collection('losses')\n",
    "\tloss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "\t# Attach a scalar summary to all individual losses and the total loss; do the\n",
    "\t# same for the averaged version of the losses.\n",
    "\tfor l in losses + [total_loss]:\n",
    "\t\t# Name each loss as '(raw)' and name the moving average version of the loss\n",
    "\t\t# as the original loss name.\n",
    "\t\ttf.summary.scalar(l.op.name + '_raw', l)\n",
    "\t\ttf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "\treturn loss_averages_op\n",
    "\n",
    "\n",
    "def train(total_loss, global_step):\n",
    "\t# Variables that affect learning rate.\n",
    "\tnum_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "\tdecay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\t# Decay the learning rate exponentially based on the number of steps.\n",
    "\tlr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "\t\t\t\t\t\t\t\t\tglobal_step,\n",
    "\t\t\t\t\t\t\t\t\tdecay_steps,\n",
    "\t\t\t\t\t\t\t\t\tLEARNING_RATE_DECAY_FACTOR,\n",
    "\t\t\t\t\t\t\t\t\tstaircase=True)\n",
    "\ttf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "\t# Generate moving averages of all losses and associated summaries.\n",
    "\tloss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "\t# Compute gradients.\n",
    "\twith tf.control_dependencies([loss_averages_op]):\n",
    "\t\topt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.2)\n",
    "\t\tgrads = opt.compute_gradients(total_loss, var_list=tf.trainable_variables())\n",
    "\n",
    "\t# Apply gradients.\n",
    "\tapply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "\t# Add histograms for trainable variables.\n",
    "\tfor var in tf.trainable_variables():\n",
    "\t\ttf.summary.histogram(var.op.name, var)\n",
    "\n",
    "\t# Add histograms for gradients.\n",
    "\t# for grad, var in grads:\n",
    "\t#     if grad is not None:\n",
    "\t#         tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "\t# Track the moving averages of all trainable variables.\n",
    "\tvariable_averages = tf.train.ExponentialMovingAverage(\n",
    "\t\tMOVING_AVERAGE_DECAY, global_step)\n",
    "\tvariables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "\twith tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "\t\ttrain_op = tf.no_op(name='train')\n",
    "\n",
    "\treturn train_op\n",
    "\n",
    "\n",
    "def get_show_preds(s_fuse):\n",
    "\t# Index 1 of fuse layers correspond to foreground, so discard index 0.\n",
    "\t_, s_logits = tf.split(tf.cast(tf.nn.softmax(s_fuse), tf.float32), 2, 3)\n",
    "\n",
    "\ttf.summary.image('s_logits', s_logits)\n",
    "\treturn s_logits\n",
    "\n",
    "\n",
    "def get_show_labels(labels):\n",
    "\ts_labels = labels\n",
    "\ts_labels = tf.cast(s_labels, tf.float32)\n",
    "\n",
    "\ttf.summary.image('s_labels', s_labels)\n",
    "\treturn s_labels\n",
    "\n",
    "\n",
    "def get_dice_coef(logits, labels):\n",
    "\tsmooth = 1e-5\n",
    "\tinter = tf.reduce_sum(tf.multiply(logits, labels))\n",
    "\tl = tf.reduce_sum(logits)\n",
    "\tr = tf.reduce_sum(labels)\n",
    "\treturn tf.reduce_mean((2.0 * inter + smooth) / (l + r + smooth))\n",
    "\n",
    "\n",
    "def dice_op(s_fuse, labels):\n",
    "\ts_logits = get_show_preds(s_fuse)\n",
    "\ts_labels = get_show_labels(labels)\n",
    "\n",
    "\ts_dice = get_dice_coef(s_logits, s_labels)\n",
    "\n",
    "\treturn s_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating csv\n",
    "\n",
    "from data_input import gen_csv_paths\n",
    "\n",
    "gen_csv_paths('reshaped_warwick/', 'train', 0)\n",
    "gen_csv_paths('reshaped_warwick/', 'train', 1)\n",
    "gen_csv_paths('reshaped_warwick/', 'train', 2)\n",
    "gen_csv_paths('reshaped_warwick/', 'train', 3)\n",
    "gen_csv_paths('reshaped_warwick/', 'test')\n",
    "gen_csv_paths('reshaped_warwick/', 'train_eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 17:24:50.424220 139864161822528 deprecation.py:323] From <ipython-input-1-df2c1ce4443c>:33: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "W0912 17:24:50.429354 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/assignement/data_input.py:148: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0912 17:24:50.434712 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0912 17:24:50.437178 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "W0912 17:24:50.442324 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0912 17:24:50.445202 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0912 17:24:50.450835 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/assignement/data_input.py:62: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "W0912 17:24:50.453589 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/data_input.py:65: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "W0912 17:24:50.456536 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/data_input.py:68: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\n",
      "\n",
      "W0912 17:24:50.482995 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0912 17:24:50.487956 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/assignement/data_input.py:99: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "W0912 17:24:50.502484 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/data_input.py:102: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0912 17:24:50.507253 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:351: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0912 17:24:50.508730 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/assignement/mainutils.py:352: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0912 17:24:50.599488 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/assignement/mainutils.py:353: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0912 17:24:50.648249 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/assignement/mainutils.py:355: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0912 17:24:50.649677 139864161822528 deprecation.py:506] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0912 17:24:51.908609 139864161822528 deprecation.py:506] From /home/lucifer/acadgild/project/assignement/mainutils.py:190: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0912 17:24:51.909469 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:191: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0912 17:24:51.919028 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:85: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0912 17:24:51.921045 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:86: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0912 17:24:52.435254 139864161822528 deprecation.py:506] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "W0912 17:24:52.457948 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0912 17:24:52.467428 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:438: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "W0912 17:24:52.476070 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:408: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 17:24:52.497407 139864161822528 deprecation_wrapper.py:119] From /home/lucifer/acadgild/project/assignement/mainutils.py:450: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "W0912 17:24:53.972949 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "I0912 17:24:57.157938 139864161822528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0912 17:24:59.468716 139864161822528 monitored_session.py:240] Graph was finalized.\n",
      "I0912 17:25:03.843045 139864161822528 session_manager.py:500] Running local_init_op.\n",
      "I0912 17:25:03.968079 139864161822528 session_manager.py:502] Done running local_init_op.\n",
      "W0912 17:25:04.539808 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0912 17:25:14.189612 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt.\n",
      "W0912 17:25:15.704656 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0912 17:25:15.706075 139864161822528 saver.py:1280] Restoring parameters from /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt-0\n",
      "I0912 17:25:17.175989 139864161822528 <ipython-input-1-df2c1ce4443c>:107] Model restored from file: /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:25:17.174074: step 0, loss = 0.41 (10.7 examples/sec; 0.094 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 17:25:19.035815 139864161822528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "I0912 17:26:24.239034 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49105\n",
      "I0912 17:27:27.278023 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.58632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:27:27.281356: step 200, loss = 0.36 (1.5 examples/sec; 0.651 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:28:30.366999 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.58506\n",
      "I0912 17:29:33.663018 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.57988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:29:33.666975: step 400, loss = 0.24 (1.6 examples/sec; 0.632 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:30:38.649169 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53879\n",
      "I0912 17:31:43.858231 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:31:43.861967: step 600, loss = 0.24 (1.5 examples/sec; 0.651 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:32:49.146182 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53167\n",
      "I0912 17:33:54.540217 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:33:54.544542: step 800, loss = 0.15 (1.5 examples/sec; 0.653 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:35:00.279367 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52116\n",
      "I0912 17:35:16.030874 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 924 into /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt.\n",
      "I0912 17:36:06.323167 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:36:06.326448: step 1000, loss = 0.22 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:37:12.226076 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51738\n",
      "I0912 17:38:17.761848 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:38:17.767640: step 1200, loss = 0.11 (1.5 examples/sec; 0.657 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:39:23.258714 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52679\n",
      "I0912 17:40:29.326674 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:40:29.331057: step 1400, loss = 0.10 (1.5 examples/sec; 0.658 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:41:35.458036 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51214\n",
      "I0912 17:42:41.045516 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:42:41.050056: step 1600, loss = 0.08 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:43:46.732173 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52238\n",
      "I0912 17:44:52.834380 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:44:52.838697: step 1800, loss = 0.09 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:45:16.368922 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 1836 into /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt.\n",
      "I0912 17:45:59.222572 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.50629\n",
      "I0912 17:47:05.178580 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:47:05.182212: step 2000, loss = 0.07 (1.5 examples/sec; 0.662 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:48:11.775951 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.50156\n",
      "I0912 17:49:18.009661 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:49:18.015855: step 2200, loss = 0.08 (1.5 examples/sec; 0.664 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:50:23.752557 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52108\n",
      "I0912 17:51:29.497397 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:51:29.502193: step 2400, loss = 0.06 (1.5 examples/sec; 0.657 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:52:35.259493 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52063\n",
      "I0912 17:53:43.851498 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.4579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:53:43.856204: step 2600, loss = 0.05 (1.5 examples/sec; 0.672 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:54:50.028050 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51111\n",
      "I0912 17:55:17.008165 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 2741 into /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt.\n",
      "I0912 17:55:56.715714 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:55:56.719505: step 2800, loss = 0.02 (1.5 examples/sec; 0.664 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:57:02.738620 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51463\n",
      "I0912 17:58:08.679827 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 17:58:08.683669: step 3000, loss = 0.11 (1.5 examples/sec; 0.660 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 17:59:14.470108 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51998\n",
      "I0912 18:00:20.185164 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:00:20.189213: step 3200, loss = 0.05 (1.5 examples/sec; 0.658 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:01:25.969588 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52012\n",
      "I0912 18:02:32.029958 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:02:32.035548: step 3400, loss = 0.01 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:03:37.873752 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51875\n",
      "I0912 18:04:44.132867 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.50923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:04:44.138780: step 3600, loss = 0.04 (1.5 examples/sec; 0.661 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:05:17.278228 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 3649 into /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt.\n",
      "I0912 18:05:51.964880 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.47423\n",
      "I0912 18:06:57.918634 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:06:57.922543: step 3800, loss = 0.05 (1.5 examples/sec; 0.669 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:08:03.730782 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51948\n",
      "I0912 18:09:09.673987 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51646\n",
      "I0912 18:09:09.681080 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 4000 into /home/lucifer/acadgild/project/assignement/warwick_train_0/model.ckpt.\n",
      "W0912 18:09:09.839879 139864161822528 deprecation.py:323] From /home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:09:09.680329: step 4000, loss = 0.03 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:09:17.893699 139864161822528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0912 18:09:20.056047 139864161822528 monitored_session.py:240] Graph was finalized.\n",
      "I0912 18:09:24.457192 139864161822528 session_manager.py:500] Running local_init_op.\n",
      "I0912 18:09:24.611781 139864161822528 session_manager.py:502] Done running local_init_op.\n",
      "I0912 18:09:34.939308 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt.\n",
      "I0912 18:09:36.490299 139864161822528 saver.py:1280] Restoring parameters from /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt-0\n",
      "I0912 18:09:37.924592 139864161822528 <ipython-input-1-df2c1ce4443c>:107] Model restored from file: /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:09:37.922978: step 0, loss = 0.18 (10.6 examples/sec; 0.095 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 18:09:39.725446 139864161822528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "I0912 18:10:45.860387 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.47191\n",
      "I0912 18:11:54.264440 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.4619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:11:54.268363: step 200, loss = 0.29 (1.5 examples/sec; 0.682 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:12:59.982368 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52166\n",
      "I0912 18:14:07.042312 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.4912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:14:07.045948: step 400, loss = 0.20 (1.5 examples/sec; 0.664 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:15:13.928629 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49507\n",
      "I0912 18:16:21.767361 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.47409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:16:21.774134: step 600, loss = 0.28 (1.5 examples/sec; 0.674 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:17:41.706960 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.25094\n",
      "I0912 18:18:50.832779 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.44664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:18:50.836739: step 800, loss = 0.15 (1.3 examples/sec; 0.745 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:19:36.488946 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 867 into /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt.\n",
      "I0912 18:19:59.344767 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.4596\n",
      "I0912 18:21:05.506897 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:21:05.510819: step 1000, loss = 0.08 (1.5 examples/sec; 0.673 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:22:11.423194 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51708\n",
      "I0912 18:23:17.648280 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:23:17.652550: step 1200, loss = 0.13 (1.5 examples/sec; 0.661 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:24:23.367108 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52163\n",
      "I0912 18:25:29.393039 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:25:29.397591: step 1400, loss = 0.10 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:26:35.480918 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51314\n",
      "I0912 18:27:41.721378 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.50965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:27:41.725161: step 1600, loss = 0.17 (1.5 examples/sec; 0.662 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:28:47.664400 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51646\n",
      "I0912 18:29:37.086020 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 1775 into /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt.\n",
      "I0912 18:29:54.465734 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:29:54.469437: step 1800, loss = 0.07 (1.5 examples/sec; 0.664 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:31:00.603075 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51201\n",
      "I0912 18:32:06.329171 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:32:06.332596: step 2000, loss = 0.04 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:33:12.423039 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.513\n",
      "I0912 18:34:18.308177 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:34:18.312622: step 2200, loss = 0.16 (1.5 examples/sec; 0.660 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:35:24.191792 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51783\n",
      "I0912 18:36:30.143718 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:36:30.148383: step 2400, loss = 0.12 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:37:35.715976 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52504\n",
      "I0912 18:38:41.552208 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:38:41.557212: step 2600, loss = 0.07 (1.5 examples/sec; 0.657 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:39:37.163770 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 2684 into /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt.\n",
      "I0912 18:39:48.518035 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.4933\n",
      "I0912 18:40:53.879274 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:40:53.884075: step 2800, loss = 0.04 (1.5 examples/sec; 0.662 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:41:59.898700 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51471\n",
      "I0912 18:43:05.603783 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:43:05.607432: step 3000, loss = 0.07 (1.5 examples/sec; 0.659 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:44:11.231883 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52374\n",
      "I0912 18:45:16.921990 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:45:16.925531: step 3200, loss = 0.01 (1.5 examples/sec; 0.657 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:46:22.670328 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52095\n",
      "I0912 18:47:28.387248 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:47:28.390733: step 3400, loss = 0.03 (1.5 examples/sec; 0.657 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:48:33.611038 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53318\n",
      "I0912 18:49:37.378732 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 3596 into /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt.\n",
      "I0912 18:49:40.909521 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.48592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:49:40.913729: step 3600, loss = 0.03 (1.5 examples/sec; 0.663 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:50:46.703555 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51989\n",
      "I0912 18:51:51.974758 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:51:51.978545: step 3800, loss = 0.06 (1.5 examples/sec; 0.655 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:52:57.100337 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5355\n",
      "I0912 18:54:02.690418 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52462\n",
      "I0912 18:54:02.696448 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 4000 into /home/lucifer/acadgild/project/assignement/warwick_train_1/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:54:02.695817: step 4000, loss = 0.01 (1.5 examples/sec; 0.654 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:54:10.974797 139864161822528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0912 18:54:13.299499 139864161822528 monitored_session.py:240] Graph was finalized.\n",
      "I0912 18:54:17.695363 139864161822528 session_manager.py:500] Running local_init_op.\n",
      "I0912 18:54:17.851568 139864161822528 session_manager.py:502] Done running local_init_op.\n",
      "I0912 18:54:27.612761 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt.\n",
      "I0912 18:54:29.190539 139864161822528 saver.py:1280] Restoring parameters from /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt-0\n",
      "I0912 18:54:30.651235 139864161822528 <ipython-input-1-df2c1ce4443c>:107] Model restored from file: /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:54:30.649451: step 0, loss = 0.44 (10.8 examples/sec; 0.092 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 18:54:32.437585 139864161822528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "I0912 18:55:37.556836 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49456\n",
      "I0912 18:56:42.163494 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:56:42.168435: step 200, loss = 0.17 (1.5 examples/sec; 0.658 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:57:46.988279 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54262\n",
      "I0912 18:58:51.883449 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 18:58:51.888108: step 400, loss = 0.25 (1.5 examples/sec; 0.649 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 18:59:57.129262 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53266\n",
      "I0912 19:01:02.976273 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:01:02.980077: step 600, loss = 0.18 (1.5 examples/sec; 0.655 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:02:09.132227 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51158\n",
      "I0912 19:03:15.037075 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:03:15.041771: step 800, loss = 0.40 (1.5 examples/sec; 0.660 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:04:21.014372 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51567\n",
      "I0912 19:04:29.485912 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 913 into /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt.\n",
      "I0912 19:05:28.384842 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.48433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:05:28.388698: step 1000, loss = 0.15 (1.5 examples/sec; 0.667 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:06:34.528753 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51186\n",
      "I0912 19:07:42.223408 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.47722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:07:42.227563: step 1200, loss = 0.14 (1.5 examples/sec; 0.669 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:08:49.770906 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.48044\n",
      "I0912 19:09:56.680163 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:09:56.684263: step 1400, loss = 0.06 (1.5 examples/sec; 0.672 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:11:02.461222 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52019\n",
      "I0912 19:12:07.978269 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:12:07.982799: step 1600, loss = 0.23 (1.5 examples/sec; 0.656 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:13:13.959265 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51559\n",
      "I0912 19:14:19.473906 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:14:19.478664: step 1800, loss = 0.11 (1.5 examples/sec; 0.657 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:14:29.992621 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 1816 into /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt.\n",
      "I0912 19:15:25.611422 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.512\n",
      "I0912 19:16:31.593595 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:16:31.597018: step 2000, loss = 0.04 (1.5 examples/sec; 0.661 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:17:37.132290 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52582\n",
      "I0912 19:18:41.903757 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:18:41.907907: step 2200, loss = 0.08 (1.5 examples/sec; 0.652 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:19:46.368336 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55124\n",
      "I0912 19:20:50.655253 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:20:50.659792: step 2400, loss = 0.08 (1.6 examples/sec; 0.644 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:21:55.349834 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54572\n",
      "I0912 19:23:00.253135 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:23:00.257041: step 2600, loss = 0.06 (1.5 examples/sec; 0.648 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:24:04.930921 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54613\n",
      "I0912 19:24:30.137863 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 2739 into /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt.\n",
      "I0912 19:25:10.440307 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:25:10.443725: step 2800, loss = 0.08 (1.5 examples/sec; 0.651 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:26:15.116198 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54617\n",
      "I0912 19:27:19.305200 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:27:19.310452: step 3000, loss = 0.10 (1.6 examples/sec; 0.644 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:28:23.972802 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54637\n",
      "I0912 19:29:28.827070 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:29:28.831376: step 3200, loss = 0.05 (1.5 examples/sec; 0.648 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:30:32.807758 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.56297\n",
      "I0912 19:31:36.622758 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.56703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:31:36.627300: step 3400, loss = 0.05 (1.6 examples/sec; 0.639 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:32:41.052452 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55208\n",
      "I0912 19:33:45.195229 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:33:45.199399: step 3600, loss = 0.06 (1.6 examples/sec; 0.643 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:34:30.677091 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 3671 into /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt.\n",
      "I0912 19:34:50.310811 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53573\n",
      "I0912 19:35:54.518195 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:35:54.522798: step 3800, loss = 0.07 (1.5 examples/sec; 0.647 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:36:58.847066 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55451\n",
      "I0912 19:38:03.363099 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55\n",
      "I0912 19:38:03.371920 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 4000 into /home/lucifer/acadgild/project/assignement/warwick_train_2/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:38:03.370792: step 4000, loss = 0.03 (1.6 examples/sec; 0.644 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:38:11.624352 139864161822528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0912 19:38:14.005091 139864161822528 monitored_session.py:240] Graph was finalized.\n",
      "I0912 19:38:18.669979 139864161822528 session_manager.py:500] Running local_init_op.\n",
      "I0912 19:38:18.826172 139864161822528 session_manager.py:502] Done running local_init_op.\n",
      "I0912 19:38:29.565835 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt.\n",
      "I0912 19:38:31.127743 139864161822528 saver.py:1280] Restoring parameters from /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt-0\n",
      "I0912 19:38:32.571798 139864161822528 <ipython-input-1-df2c1ce4443c>:107] Model restored from file: /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:38:32.569693: step 0, loss = 0.30 (10.2 examples/sec; 0.098 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 19:38:34.373224 139864161822528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "I0912 19:39:38.735226 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.51133\n",
      "I0912 19:40:42.674745 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.56398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:40:42.679389: step 200, loss = 0.30 (1.5 examples/sec; 0.651 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:41:45.527471 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.59102\n",
      "I0912 19:42:49.397316 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.56568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:42:49.401438: step 400, loss = 0.20 (1.6 examples/sec; 0.634 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:43:54.203070 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54307\n",
      "I0912 19:44:59.296015 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:44:59.301307: step 600, loss = 0.26 (1.5 examples/sec; 0.649 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:46:04.065209 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54394\n",
      "I0912 19:47:08.804590 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:47:08.808689: step 800, loss = 0.22 (1.5 examples/sec; 0.648 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:48:13.503902 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54561\n",
      "I0912 19:48:31.602340 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 928 into /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt.\n",
      "I0912 19:49:17.744019 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:49:17.748055: step 1000, loss = 0.09 (1.6 examples/sec; 0.645 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:50:21.351658 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.57214\n",
      "I0912 19:51:25.797312 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:51:25.802073: step 1200, loss = 0.15 (1.6 examples/sec; 0.640 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:52:32.796645 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.49255\n",
      "I0912 19:53:40.696564 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.47276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:53:40.701294: step 1400, loss = 0.08 (1.5 examples/sec; 0.674 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:54:45.833524 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53523\n",
      "I0912 19:55:50.846318 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:55:50.850419: step 1600, loss = 0.07 (1.5 examples/sec; 0.651 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:56:55.789229 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53981\n",
      "I0912 19:58:00.445629 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 19:58:00.450921: step 1800, loss = 0.11 (1.5 examples/sec; 0.648 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 19:58:32.122377 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 1849 into /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt.\n",
      "I0912 19:59:05.834006 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52932\n",
      "I0912 20:00:10.279681 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:00:10.283825: step 2000, loss = 0.09 (1.5 examples/sec; 0.649 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:01:14.959182 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54609\n",
      "I0912 20:02:19.753003 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:02:19.757294: step 2200, loss = 0.04 (1.5 examples/sec; 0.647 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:03:24.111983 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55379\n",
      "I0912 20:04:27.805194 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.57003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:04:27.808824: step 2400, loss = 0.01 (1.6 examples/sec; 0.640 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:05:32.215597 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55254\n",
      "I0912 20:06:36.615389 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.5528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:06:36.619478: step 2600, loss = 0.03 (1.6 examples/sec; 0.644 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:07:41.174842 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54896\n",
      "I0912 20:08:32.759591 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 2780 into /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt.\n",
      "I0912 20:08:46.678236 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.52664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:08:46.681344: step 2800, loss = 0.07 (1.5 examples/sec; 0.650 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:09:51.817221 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.53518\n",
      "I0912 20:10:56.122418 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.55509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:10:56.128380: step 3000, loss = 0.05 (1.5 examples/sec; 0.647 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:12:00.916965 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54334\n",
      "I0912 20:13:05.783903 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.54162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:13:05.788336: step 3200, loss = 0.10 (1.5 examples/sec; 0.648 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:14:09.417697 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.57149\n",
      "I0912 20:15:12.303824 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.59018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:15:12.307824: step 3400, loss = 0.01 (1.6 examples/sec; 0.633 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:16:15.525165 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.58175\n",
      "I0912 20:17:19.025546 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.57479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:17:19.029340: step 3600, loss = 0.01 (1.6 examples/sec; 0.634 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:18:20.907828 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.61597\n",
      "I0912 20:18:32.778784 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 3720 into /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt.\n",
      "I0912 20:19:20.795271 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.6698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:19:20.798842: step 3800, loss = 0.03 (1.6 examples/sec; 0.609 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0912 20:20:19.849335 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.69336\n",
      "I0912 20:21:19.278315 139864161822528 basic_session_run_hooks.py:692] global_step/sec: 1.68268\n",
      "I0912 20:21:19.284389 139864161822528 basic_session_run_hooks.py:606] Saving checkpoints for 4000 into /home/lucifer/acadgild/project/assignement/warwick_train_3/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-12 20:21:19.283760: step 4000, loss = 0.06 (1.7 examples/sec; 0.592 sec/batch)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucifer/acadgild/project/test/testenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# binary to train using a single GPU\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "import mainutils\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/home/lucifer/acadgild/project/assignement/warwick_train',\n",
    "\t\t\t\t\t\t   \"\"\"Directory where to write event logs \"\"\"\n",
    "\t\t\t\t\t\t   \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_string('eval_data', 'train',\n",
    "\t\t\t\t\t\t   \"\"\"Either 'test' or 'train'.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 4000,\n",
    "\t\t\t\t\t\t\t\"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "\t\t\t\t\t\t\t\"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 200,\n",
    "\t\t\t\t\t\t\t\"\"\"How often to log results to the console.\"\"\")\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "\n",
    "def train(sessid):\n",
    "\t\"\"\"Train BBBC006 for a number of steps.\"\"\"\n",
    "\twith tf.Graph().as_default():\n",
    "\t\tckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "\t\tglobal_step_init = -1\n",
    "\t\tglobal_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\t\tif ckpt and ckpt.model_checkpoint_path:\n",
    "\t\t\t# Restores from checkpoint\n",
    "\t\t\tglobal_step_init = int(ckpt.model_checkpoint_path.split('/')[-1]\n",
    "\t\t\t\t\t\t\t\t   .split('-')[-1])\n",
    "\n",
    "\t\t# Get images and labels for BBBC006.\n",
    "\t\t# Force input pipeline to CPU:0 to avoid operations sometimes ending up\n",
    "\t\t# on GPU and resulting in a slow down.\n",
    "\t\timages, labels, i_paths = mainutils.inputs(eval_data=FLAGS.eval_data,sessid=sessid)\n",
    "\t\t# Build a Graph that computes the logits predictions from the\n",
    "\t\t# inference model.\n",
    "\t\t# s_fuse = mainutils.inference(images)\n",
    "\t\ts_fuse, encoding = mainutils.inference_bottleneck(images)\n",
    "\t\tmainutils.get_show_preds(s_fuse)\n",
    "\n",
    "\t\t# Calculate loss.\n",
    "\t\tsegments_labels = labels\n",
    "\t\twith tf.variable_scope('{}_cross_entropy'.format('s')) as scope:\n",
    "\t\t\tclass_prop = mainutils.S_CLASS_PROP\n",
    "\t\t\tweight_per_label = tf.scalar_mul(class_prop, tf.cast(tf.equal(segments_labels, 0),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.float32)) + \\\n",
    "\t\t\t\t\t\t\t   tf.scalar_mul(1.0 - class_prop, tf.cast(tf.equal(segments_labels, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   tf.float32))\n",
    "\t\t\tcross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "\t\t\t\tlabels=tf.squeeze(segments_labels, squeeze_dims=[3]), logits=s_fuse)\n",
    "\t\t\tcross_entropy_weighted = tf.multiply(weight_per_label, cross_entropy)\n",
    "\t\t\tcross_entropy_mean = tf.reduce_mean(cross_entropy_weighted, name=scope.name)\n",
    "\n",
    "\t\tloss = cross_entropy_mean\n",
    "\n",
    "\t\t# Build a Graph that trains the model with one batch of examples and\n",
    "\t\t# updates the model parameters.\n",
    "\t\ttrain_op = mainutils.train(loss, global_step)\n",
    "\n",
    "\t\tclass _LoggerHook(tf.train.SessionRunHook):\n",
    "\t\t\t\"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "\t\t\tdef begin(self):\n",
    "\t\t\t\tself._step = global_step_init\n",
    "\t\t\t\tself._start_time = time.time()\n",
    "\n",
    "\t\t\tdef before_run(self, run_context):\n",
    "\t\t\t\tself._step += 1\n",
    "\t\t\t\treturn tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "\t\t\tdef after_run(self, run_context, run_values):\n",
    "\t\t\t\tif self._step % FLAGS.log_frequency == 0:\n",
    "\t\t\t\t\tcurrent_time = time.time()\n",
    "\t\t\t\t\tduration = current_time - self._start_time\n",
    "\t\t\t\t\tself._start_time = current_time\n",
    "\n",
    "\t\t\t\t\tloss_value = run_values.results\n",
    "\t\t\t\t\texamples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "\t\t\t\t\tsec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "\t\t\t\t\tformat_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "\t\t\t\t\t\t\t\t  'sec/batch)')\n",
    "\t\t\t\t\tprint(format_str % (datetime.now(), self._step, loss_value,\n",
    "\t\t\t\t\t\t\t\t\t\texamples_per_sec, sec_per_batch))\n",
    "\n",
    "\t\tsaver = tf.train.Saver()\n",
    "\t\twith tf.train.MonitoredTrainingSession(\n",
    "\t\t\t\tcheckpoint_dir=FLAGS.train_dir,\n",
    "\t\t\t\thooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "\t\t\t\t\t   tf.train.NanTensorHook(loss),\n",
    "\t\t\t\t\t   _LoggerHook()],\n",
    "\t\t\t\tconfig=tf.ConfigProto(\n",
    "\t\t\t\t\tlog_device_placement=FLAGS.log_device_placement)\n",
    "\t\t\t\t) as mon_sess:\n",
    "\t\t\tckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "\n",
    "\t\t\tif ckpt:\n",
    "\t\t\t\tsaver.restore(mon_sess, ckpt.model_checkpoint_path)\n",
    "\t\t\t\tlogging.info(\"Model restored from file: %s\" % ckpt.model_checkpoint_path)\n",
    "\t\t\twhile not mon_sess.should_stop():\n",
    "\t\t\t\t_,losseval = mon_sess.run([train_op,loss])\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "\tFLAGS.train_dir = '/home/lucifer/acadgild/project/assignement/warwick_train_0'\n",
    "\ttrain(0)\n",
    "\tFLAGS.train_dir = '/home/lucifer/acadgild/project/assignement/warwick_train_1'\n",
    "\ttrain(1)\n",
    "\tFLAGS.train_dir = '/home/lucifer/acadgild/project/assignement/warwick_train_2'\n",
    "\ttrain(2)\n",
    "\tFLAGS.train_dir = '/home/lucifer/acadgild/project/assignement/warwick_train_3'\n",
    "\ttrain(3)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DEFINE_string() missing 1 required positional argument: 'help'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-381571d3e653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval_dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/lucifer/acadgild/project/assignement/warwick_eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/lucifer/acadgild/project/assignement/warwick_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/acadgild/project/test/testenv/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: DEFINE_string() missing 1 required positional argument: 'help'"
     ]
    }
   ],
   "source": [
    "# Evaluation for the traind model\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import mainutils\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', '/home/lucifer/acadgild/project/assignement/warwick_eval')\n",
    "tf.app.flags.DEFINE_string('eval_data', 'train_eval')\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '/home/lucifer/acadgild/project/assignement/warwick_train')\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5)\n",
    "tf.app.flags.DEFINE_integer('num_examples', 100)\n",
    "tf.app.flags.DEFINE_boolean('run_once', True)\n",
    "\n",
    "\n",
    "def eval_once(saver, dice_op, summary_writer, summary_op, s_fuse, images, labels, i_paths, encoding, sessid):\n",
    "\n",
    "\tFLAGS.checkpoint_dir = '/home/lucifer/acadgild/project/assignement/warwick_train_'+str(sessid)\n",
    "\twith tf.Session() as sess:\n",
    "\t\tckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "\t\tif ckpt and ckpt.model_checkpoint_path:\n",
    "\t\t\t# Restores from checkpoint\n",
    "\t\t\tsaver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\t\t\tprint(\"Model restored from file: %s\" % ckpt.model_checkpoint_path)\n",
    "\t\t\tglobal_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "\t\telse:\n",
    "\t\t\tprint('No checkpoint file found')\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Start the queue runners.\n",
    "\t\tcoord = tf.train.Coordinator()\n",
    "\t\tpredictions = {}\n",
    "\t\tencodings = {}\n",
    "\t\tdice_scores = {}\n",
    "\t\ttry:\n",
    "\t\t\tthreads = []\n",
    "\t\t\tfor qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "\t\t\t\tthreads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t start=True))\n",
    "\n",
    "\t\t\tnum_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "\t\t\tavg_s_dice = 0\n",
    "\t\t\tstep = 0\n",
    "\t\t\twhile step < num_iter and not coord.should_stop():\n",
    "\t\t\t\ts_dice,i_path,s_fuse_out,encoded_image = sess.run([dice_op,i_paths,s_fuse,encoding])\n",
    "\t\t\t\tpredictions[i_path[0]] = s_fuse_out\n",
    "\t\t\t\tencodings[i_path[0]] = encoded_image\n",
    "\t\t\t\tdice_scores[i_path[0]] = s_dice\n",
    "\t\t\t\tim0 = s_fuse_out[0,:,:,0]\n",
    "\t\t\t\tim1 = s_fuse_out[0,:,:,1]\n",
    "\t\t\t\timage = (im1>im0)*128\n",
    "\t\t\t\tim = Image.fromarray(image.astype(np.uint8))\n",
    "\t\t\t\tim.save('results/'+i_path[0].split('/')[2]+'.bmp')\n",
    "\t\t\t\tavg_s_dice += s_dice\n",
    "\t\t\t\tstep += 1\n",
    "\n",
    "\t\t\tavg_s_dice /= step\n",
    "\t\t\tprint('%s: s_dice avg = %.3f' % (datetime.now(), avg_s_dice))\n",
    "\n",
    "\t\t\tsummary = tf.Summary()\n",
    "\t\t\tsummary.ParseFromString(sess.run(summary_op))\n",
    "\t\t\tsummary.value.add(tag='dice_s', simple_value=avg_s_dice)\n",
    "\t\t\tsummary_writer.add_summary(summary, global_step)\n",
    "\t\texcept Exception as e:  # pylint: disable=broad-except\n",
    "\t\t\tcoord.request_stop(e)\n",
    "\t\tif FLAGS.eval_data == 'train_eval':\n",
    "\t\t\tnp.save('train_eval_data_'+str(sessid)+'.npy',[predictions, encodings, dice_scores]) # otherwise don't save\n",
    "\t\tcoord.request_stop()\n",
    "\t\tcoord.join(threads, stop_grace_period_secs=20)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "\t\"\"\"Eval BBBC006 for a number of steps.\"\"\"\n",
    "\tfor sessid in range(4):\n",
    "\t\tFLAGS.eval_dir = '/home/lucifer/acadgild/project/assignement/warwick_eval_' + str(sessid)\n",
    "\t\twith tf.Graph().as_default() as g:\n",
    "\t\t\t# Get images and labels for BBBC006.\n",
    "\t\t\timages, labels, i_paths = mainutils.inputs(eval_data=FLAGS.eval_data, sessid=sessid)\n",
    "\t\t\t\n",
    "\t\t\t# Build a Graph that computes the logits predictions from the\n",
    "\t\t\t# inference model.\n",
    "\t\t\t# s_fuse = mainutils.inference(images, train=False)\n",
    "\t\t\ts_fuse, encoding = mainutils.inference_bottleneck(images, train=False)\n",
    "\n",
    "\t\t\tdice_op = mainutils.dice_op(s_fuse, labels)\n",
    "\n",
    "\t\t\t# Restore the moving average version of the learned variables for eval.\n",
    "\t\t\tvariable_averages = tf.train.ExponentialMovingAverage(\n",
    "\t\t\t\tmainutils.MOVING_AVERAGE_DECAY)\n",
    "\t\t\tvariables_to_restore = variable_averages.variables_to_restore()\n",
    "\t\t\tsaver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "\t\t\t# Build the summary operation based on the TF collection of Summaries.\n",
    "\t\t\tsummary_op = tf.summary.merge_all()\n",
    "\t\t\tsummary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n",
    "\t\t\ts_fuse_softmax = tf.nn.softmax(s_fuse)\n",
    "\n",
    "\t\t\twhile True:\n",
    "\t\t\t\teval_once(saver, dice_op, summary_writer, summary_op, s_fuse_softmax, images, labels, i_paths, encoding, sessid)\n",
    "\t\t\t\tif FLAGS.run_once:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\ttime.sleep(FLAGS.eval_interval_secs)\n",
    "\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "\tif tf.gfile.Exists(FLAGS.eval_dir):\n",
    "\t\ttf.gfile.DeleteRecursively(FLAGS.eval_dir)\n",
    "\ttf.gfile.MakeDirs(FLAGS.eval_dir)\n",
    "\tevaluate()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
